# Do We Need VLMs for Action Spotting?
## Authors:Ritabrata Chakraborty, Sandeep Chaurasia
Manipal University Jaipur, India

## Paper Status: Currently under submission for ACL SRW 2025

Abstract:
The paper explores the necessity of Vision-Language Models (VLMs) for the task of action spotting in sports events, particularly in soccer. Traditionally, action spotting has heavily relied on visual cues from video data to identify and label events such as goals, fouls, and penalties. However, the emergence of advanced Large Language Models (LLMs) capable of understanding textual commentary has raised the question: Do we need VLMs for action spotting when LLMs, with the appropriate context, can provide comparable or superior performance? This research investigates whether VLMs are required for action spotting tasks or if text-based approaches (such as LLMs) can achieve similar or better results.

Key Objectives:
Investigate LLMs vs VLMs: The paper aims to explore whether LLMs can perform action spotting from timestamped commentary and replace VLMs in this task.

Evaluate Action Spotting Performance: The research focuses on evaluating LLMs' ability to detect actions based on commentary alone, without relying on visual data, which traditionally forms the core of action spotting tasks.

Examine the Role of Fine-grained Descriptions: We explore whether fine-grained textual descriptions of events can effectively capture the nuances of key moments in a match, similar to the insights provided by visual frames in video data.

Efficiency and Scalability: The paper also evaluates the computational efficiency and scalability of LLM-based action spotting systems in comparison to traditional VLM-based approaches, considering real-time applications.

